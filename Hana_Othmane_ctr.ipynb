{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13838820,"sourceType":"datasetVersion","datasetId":8770842}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß† Multimodal CTR Prediction (MM-CTR)\n1. Vue d‚Äôensemble\n   \n- Task 1 ‚Äì Multimodal Item Embedding G√©n√©ration de repr√©sentations items multimodales 128-d √† partir du contenu visuel et du comportement utilisateur.\n\n- Task 1&2 ‚Äì CTR Prediction Entra√Ænement d‚Äôun mod√®le DIN (Deep Interest Network) avec des embeddings items fig√©s, conform√©ment aux r√®gles du challenge.\n\nL‚Äôobjectif est d‚Äôam√©liorer la performance CTR (AUC) par rapport au baseline fourni, tout en respectant strictement l‚Äôint√©grit√© des donn√©es.","metadata":{}},{"cell_type":"markdown","source":"# üìò Task 1 : Multimodal Item Embedding\n\n## 1.1 Motivation & ApprocheLe\nbaseline classique (concat√©nation Texte+Image suivie d'une PCA) √©choue √† capturer la structure comportementale implicite. Notre pipeline r√©sout cela via une distillation de connaissances : nous enseignons √† un encodeur visuel √† pr√©dire le contexte d'utilisation de l'item.\n\n## 1.2 Extraction Visuelle (Teacher)\nNous utilisons CLIP (Contrastive Language-Image Pre-training) pour extraire la s√©mantique visuelle brute.\n- Mod√®le : CLIP ViT-B/32 (OpenAI).\n- Processus :\n  1. Backbone vision gel√© (frozen).\n  2. Projection visuelle + Normalisation L2.\n  3. Traitement du Cold-Start (image par d√©faut si absente).\n- Sortie : Vecteur s√©mantique de dimension $d=512$.\n\n## 1.3 Apprentissage Comportemental (Target)\nNous construisons l'espace latent cible en analysant les s√©quences d'interactions dans item_seq.parquet.\n- M√©thode : Word2Vec (Skip-Gram).\n- Analogie : S√©quence utilisateur $\\approx$ Phrase ; Item $\\approx$ Mot.\n- Hyperparam√®tres : Dimension $d=128$, Fen√™tre $= 5$.\n- R√©sultat : Ces embeddings capturent les co-occurrences et la substituabilit√© des items.\n\n## 1.4 Distillation Multimodale (Le Pont)\nC'est l'√©tape critique o√π l'information visuelle est align√©e sur l'espace comportemental.\n- Alignement : Intersection des items (CLIP $\\cap$ W2V) via recherche binaire acc√©l√©r√©e (Numba).\n- Architecture du Projecteur (MLP) :$$512 \\xrightarrow{\\text{Dense}} 512 \\xrightarrow{\\text{BN, ReLU, Drop}} 256 \\xrightarrow{\\text{Dense}} 128$$\n- Objectif d'entra√Ænement :Minimiser la distance (MSE) entre la projection de l'image et l'embedding comportemental appris.\n\n## 1.5 G√©n√©ration Finale\n- Inf√©rence : Image $\\to$ CLIP $\\to$ Projecteur $\\to$ Embedding (128-d).\n- Sortie : Mise √† jour de item_info.parquet (colonne item_emb) sans alt√©rer les autres donn√©es.","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPProcessor, CLIPModel\nfrom gensim.models import Word2Vec\nimport datasets\nfrom numba import njit, prange\n\n# ==========================================\n# 0. CONFIGURATION\n# ==========================================\nclass Config:\n    # Chemins\n    DATA_DIR = \"/kaggle/input/data-ctr\"\n        \n    ITEM_INFO_PATH = \"/kaggle/input/data-ctr/item_info.parquet\"\n    SEQ_PATH = \"/kaggle/input/data-ctr/item_seq.parquet\"\n    IMAGE_DIR = \"/kaggle/input/data-ctr/item_images/item_images\"\n    \n    # Mod√®le\n    MODEL_ID = \"openai/clip-vit-base-patch32\"\n    INPUT_DIM = 512\n    TARGET_DIM = 128\n    \n    # Entra√Ænement Projecteur\n    BATCH_SIZE = 128\n    NUM_WORKERS = 2\n    PROJ_LR = 1e-3\n    PROJ_EPOCHS = 10\n    \n    # Word2Vec\n    LIMIT_W2V_DATA = False \n    \n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    OUTPUT_PATH = \"item_emb_projector.parquet\"\n    OUTPUT_INFO_UPDATED = \"item_info_updated.parquet\"\n\ndevice = Config.DEVICE\nprint(f\"Running on {device}\")\n\n# ==========================================\n# 1. HELPER CLASSES (Optimized)\n# ==========================================\n\n# --- NUMBA ALIGNMENT KERNEL ---\n@njit(parallel=True)\ndef numba_fast_alignment(clip_ids, w2v_ids_sorted, w2v_vecs_sorted, dim):\n    n = len(clip_ids)\n    \n    # Output arrays\n    # mask: 1 if found, 0 if not\n    mask = np.zeros(n, dtype=np.bool_)\n    # targets: vectors aligned with clip_ids\n    targets = np.zeros((n, dim), dtype=np.float32)\n    \n    # Parallel Loop\n    for i in prange(n):\n        target_id = clip_ids[i]\n        \n        # Binary search manually or use np.searchsorted logic\n        # Since we are inside Numba, np.searchsorted is supported and fast\n        idx = np.searchsorted(w2v_ids_sorted, target_id)\n        \n        # Check if found\n        if idx < len(w2v_ids_sorted) and w2v_ids_sorted[idx] == target_id:\n            mask[i] = True\n            targets[i] = w2v_vecs_sorted[idx]\n            \n    return mask, targets\n\n# --- GENERATOR WITH TQDM ---\nclass SequenceGenerator:\n    def __init__(self, sequences, total_len=None):\n        self.sequences = sequences\n        self.total_len = total_len\n        \n    def __iter__(self):\n        # On wrap l'it√©rateur avec tqdm pour voir la vitesse de lecture\n        iterator = self.sequences\n        if self.total_len:\n            iterator = tqdm(self.sequences, total=self.total_len, desc=\"Stream W2V Seq\")\n            \n        for seq in iterator:\n            if seq is not None:\n                yield [str(x) for x in seq]\n\n# --- CLIP & PROJECTOR ---\nclass CLIPWrapper(nn.Module):\n    def __init__(self, model_id):\n        super().__init__()\n        self.model = CLIPModel.from_pretrained(model_id)\n        self.model.vision_model.requires_grad_(False)\n        \n    def forward(self, pixel_values):\n        vision_outputs = self.model.vision_model(pixel_values=pixel_values)\n        image_embeds = self.model.visual_projection(vision_outputs[1])\n        return image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n\nclass VisualProjector(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2), \n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, output_dim)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass ImageDataset(Dataset):\n    def __init__(self, hf_dataset, image_dir, processor):\n        self.data = hf_dataset\n        self.image_dir = image_dir\n        self.processor = processor\n        self.default_img = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n        \n    def __len__(self): return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        item_id = str(item['item_id'])\n        img_path = os.path.join(self.image_dir, f\"{item_id}.jpg\")\n        try: image = Image.open(img_path).convert(\"RGB\")\n        except: image = self.default_img\n        inputs = self.processor(images=image, return_tensors=\"pt\")\n        return {\n            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n            \"item_id\": int(item_id)\n        }\n\n# ==========================================\n# 2. MAIN PIPELINE\n# ==========================================\n\ndef main():\n    # --- PHASE 1: IMAGES (TEACHER 1) ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"PHASE 1: CLIP Feature Extraction\")\n    print(\"=\"*50)\n    \n    hf_ds = datasets.load_dataset(\"parquet\", data_files=Config.ITEM_INFO_PATH, split=\"train\")\n    processor = CLIPProcessor.from_pretrained(Config.MODEL_ID)\n    clip_model = CLIPWrapper(Config.MODEL_ID).to(device)\n    if torch.cuda.device_count() > 1: clip_model = nn.DataParallel(clip_model)\n\n    loader = DataLoader(\n        ImageDataset(hf_ds, Config.IMAGE_DIR, processor),\n        batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=Config.NUM_WORKERS, pin_memory=True\n    )\n\n    image_features_list = []\n    item_ids_list = []\n\n    # TQDM sur l'extraction\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"[1/4] Extracting CLIP\"):\n            pixel_values = batch[\"pixel_values\"].to(device)\n            ids = batch[\"item_id\"].numpy()\n            img_emb = clip_model(pixel_values)\n            image_features_list.append(img_emb.cpu().numpy())\n            item_ids_list.append(ids)\n\n    raw_image_feats = np.vstack(image_features_list)\n    all_item_ids = np.concatenate(item_ids_list)\n    \n    del clip_model, loader\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    # --- PHASE 2a: WORD2VEC (TEACHER 2) ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"PHASE 2a: Behavioral Learning (Word2Vec)\")\n    print(\"=\"*50)\n\n    df_seq = pd.read_parquet(Config.SEQ_PATH, columns=['item_seq'])\n    if Config.LIMIT_W2V_DATA:\n        seq_data = df_seq['item_seq'].iloc[:1000000]\n    else:\n        seq_data = df_seq['item_seq']\n\n    # Generator with TQDM\n    sentences_stream = SequenceGenerator(seq_data, total_len=len(seq_data))\n    \n    print(\"[2/4] Training Word2Vec...\")\n    w2v_model = Word2Vec(\n        sentences=sentences_stream, \n        vector_size=Config.TARGET_DIM, \n        window=5, \n        min_count=1, \n        workers=4,\n        epochs=3\n    )\n    \n    # --- NUMBA ALIGNMENT ---\n    print(\"\\n[INFO] Preparing Numba Alignment...\")\n    # 1. Extract Vocab to Arrays\n    vocab_keys = list(w2v_model.wv.index_to_key)\n    vocab_ids = np.array([int(k) for k in vocab_keys], dtype=np.int64)\n    vocab_vecs = np.array([w2v_model.wv[k] for k in vocab_keys], dtype=np.float32)\n    \n    # 2. Sort for Binary Search (Required for Numba speed)\n    print(\"[INFO] Sorting W2V Vocab...\")\n    sort_idx = np.argsort(vocab_ids)\n    vocab_ids_sorted = vocab_ids[sort_idx]\n    vocab_vecs_sorted = vocab_vecs[sort_idx]\n    \n    # 3. Run Numba Kernel\n    print(\"[INFO] Running Numba Fast Alignment...\")\n    mask, targets = numba_fast_alignment(\n        all_item_ids.astype(np.int64), \n        vocab_ids_sorted, \n        vocab_vecs_sorted, \n        Config.TARGET_DIM\n    )\n    \n    valid_count = np.sum(mask)\n    print(f\"‚úÖ Items Aligned: {valid_count} / {len(all_item_ids)} ({valid_count/len(all_item_ids)*100:.1f}%)\")\n    \n    # Filter Data for Training\n    X_train = raw_image_feats[mask]\n    y_train = targets[mask]\n    \n    del df_seq, sentences_stream, w2v_model, vocab_ids, vocab_vecs\n    gc.collect()\n\n    # --- PHASE 2b: PROJECTOR TRAINING ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"PHASE 2b: Projector Training (Distillation)\")\n    print(\"=\"*50)\n    \n    train_ds = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n    train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n\n    projector = VisualProjector(Config.INPUT_DIM, Config.TARGET_DIM).to(device)\n    opt = optim.Adam(projector.parameters(), lr=Config.PROJ_LR)\n    crit = nn.MSELoss()\n    \n    projector.train()\n    \n    # Loop with TQDM\n    for ep in range(Config.PROJ_EPOCHS):\n        total_loss = 0\n        pbar = tqdm(train_loader, desc=f\"Ep {ep+1}/{Config.PROJ_EPOCHS}\", leave=True)\n        \n        for bx, by in pbar:\n            bx, by = bx.to(device), by.to(device)\n            \n            opt.zero_grad()\n            pred = projector(bx)\n            loss = crit(pred, by)\n            loss.backward()\n            opt.step()\n            \n            total_loss += loss.item()\n            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n\n    # --- PHASE 3: INFERENCE ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"PHASE 3: Final Inference & Save\")\n    print(\"=\"*50)\n    \n    projector.eval()\n    final_embs_list = []\n    \n    # Infer on ALL items (Cold Start solution)\n    inf_loader = DataLoader(TensorDataset(torch.FloatTensor(raw_image_feats)), batch_size=1024, shuffle=False)\n    \n    with torch.no_grad():\n        for (bx,) in tqdm(inf_loader, desc=\"[4/4] Projecting\"):\n            bx = bx.to(device)\n            emb = projector(bx)\n            # L2 Normalize\n            emb = emb / (emb.norm(dim=-1, keepdim=True) + 1e-6)\n            final_embs_list.append(emb.cpu().numpy())\n\n    final_embs = np.vstack(final_embs_list)\n\n    # Save\n    print(f\"Saving to {Config.OUTPUT_PATH}...\")\n    df_out = pl.DataFrame({\n        \"item_id\": all_item_ids,\n        \"item_emb\": list(final_embs)\n    })\n    df_out.write_parquet(Config.OUTPUT_PATH)\n    \n    # Update item_info\n    print(\"Updating item_info.parquet...\")\n    df_info = pl.read_parquet(Config.ITEM_INFO_PATH)\n    if \"item_emb\" in df_info.columns:\n        df_info = df_info.drop(\"item_emb\")\n    \n    df_info = df_info.join(df_out, on=\"item_id\", how=\"left\")\n    df_info.write_parquet(Config.OUTPUT_INFO_UPDATED)\n    \n    print(\"\\n‚úÖ [DONE] Projector Pipeline Completed.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T20:10:37.088330Z","iopub.execute_input":"2025-12-15T20:10:37.088898Z","iopub.status.idle":"2025-12-15T20:38:37.090789Z","shell.execute_reply.started":"2025-12-15T20:10:37.088852Z","shell.execute_reply":"2025-12-15T20:38:37.090096Z"}},"outputs":[{"name":"stderr","text":"2025-12-15 20:10:49.287689: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765829449.523482      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765829449.595579      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Running on cuda\n\n==================================================\nPHASE 1: CLIP Feature Extraction\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2f7b4da17ad4ee292d2f955fa9bf55f"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"515d634d3b7149dbaf0802b03c8778fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"448792fb31414ea592b340a5b3e8f485"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04364ec82e1c475485e3e621b4e99dcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8f2c03cef1d46b2be29d3ab4b7b74d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82625dd7d3a7454296ec8078d9275c6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c3c2bf5b4dd45e8af50c1232460554d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99b68fdcbb34430c9986fe419f7357b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e94ea5bd09f42ea94ab4d77b7e8e099"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c83fee317c58445787cf970cc49f5a53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"[1/4] Extracting CLIP:   0%|          | 0/717 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57e8aad882f04048b1b6640e70b21564"}},"metadata":{}},{"name":"stdout","text":"\n==================================================\nPHASE 2a: Behavioral Learning (Word2Vec)\n==================================================\n[2/4] Training Word2Vec...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Stream W2V Seq:   0%|          | 0/6000000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d316e2bb23a41ce9364292ad951dacf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Stream W2V Seq:   0%|          | 0/6000000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1423fde4f76049e2a3e085eff27ff099"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Stream W2V Seq:   0%|          | 0/6000000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d52f1c9442e4703a07b148484bbe5e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Stream W2V Seq:   0%|          | 0/6000000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ca94e7f4fb4404fac99ef4f035697c2"}},"metadata":{}},{"name":"stdout","text":"\n[INFO] Preparing Numba Alignment...\n[INFO] Sorting W2V Vocab...\n[INFO] Running Numba Fast Alignment...\n‚úÖ Items Aligned: 91298 / 91718 (99.5%)\n\n==================================================\nPHASE 2b: Projector Training (Distillation)\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Ep 1/10:   0%|          | 0/357 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fa6e9d3a7604552b58f0404196cc0da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 2/10:   0%|          | 0/357 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d05bea7c6c764632849eba7ebbbea688"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 3/10:   0%|          | 0/357 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"137ad8b49f3247c2bb0e3f3fd3615abd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 4/10:   0%|          | 0/357 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7669a3acdb234d58aab189fbd4378433"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 5/10:   0%|          | 0/357 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"def0fcdfd9684c3d8faa392814a02dbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 6/10:   0%|          | 0/357 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f920821ebe62416cb555387e30ffd5e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 7/10:   0%|          | 0/357 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"340f128ab67f4f6caf6bd78e837dff23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 8/10:   0%|          | 0/357 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6d1c525be844f00b34a4f6ddd59e952"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 9/10:   0%|          | 0/357 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fb9b0bb40a1442c8e413726139a645a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 10/10:   0%|          | 0/357 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4500f2ec660744b9ba899707c316d080"}},"metadata":{}},{"name":"stdout","text":"\n==================================================\nPHASE 3: Final Inference & Save\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[4/4] Projecting:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a2f04b11f8040a98fdbd3497af13d8d"}},"metadata":{}},{"name":"stdout","text":"Saving to item_emb_projector.parquet...\nUpdating item_info.parquet...\n\n‚úÖ [DONE] Projector Pipeline Completed.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# üìô Task 1&2 : CTR Prediction\n\n## 2.1 Pr√©paration\nLes embeddings g√©n√©r√©s en Task 1 sont int√©gr√©s comme features denses.\n- Renommage : item_emb $\\to$ item_emb_d128.\n- Contrainte : Les poids de cet embedding sont fig√©s (non-trainable) pour isoler la performance du mod√®le CTR.\n\n## 2.2 Mod√®le : Deep Interest Network (DIN)\nPour la pr√©diction, nous utilisons DIN impl√©ment√© via FuxiCTR (PyTorch).\n- Pourquoi DIN ? Contrairement aux mod√®les statiques, DIN utilise un m√©canisme d'attention locale. Il calcule dynamiquement le poids de chaque item de l'historique utilisateur en fonction de sa pertinence par rapport √† l'item candidat actuel.\n- Entr√©es :\n  - User Profile & Context Features.\n  - User Behavior Sequence (Historique).\n  - Candidate Item (Target).\n  - Multimodal Embedding (128-d).\n\n## 2.3 Entra√Ænement & Inf√©rence\n- Training : Sur train.parquet et valid.parquet (Optimiseur Adam, Early Stopping).\n- Prediction : Sur test.parquet avec chargement du meilleur checkpoint.\n\n## 2.4 Livrable\nLe fichier de soumission prediction_task1&2.csv est g√©n√©r√© au format requis :\nID,Task2\n0,0.8123\n1,0.1345\n...","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport glob\nimport sys\nimport numpy as np\nimport pandas as pd\nimport importlib.util\nfrom tqdm.auto import tqdm\nimport torch\n\n# Numba Check\ntry:\n    from numba import njit, prange\n    NUMBA_AVAILABLE = True\nexcept ImportError:\n    NUMBA_AVAILABLE = False\n    print(\"‚ö†Ô∏è Numba non d√©tect√©. Mode JIT d√©sactiv√©.\")\n\n# ====================================================\n# 0. CONFIGURATION\n# ====================================================\nclass Config:\n    # PATH TO YOUR FILE\n    CUSTOM_ITEM_INFO_PATH = \"/kaggle/working/item_info_updated.parquet\"\n    \n    # Normalisation L2 (Recommended)\n    NORMALIZE_EMBEDDINGS = True  \n    \n    # Paths\n    SOURCE_DATA_DIR = \"/kaggle/input/data-ctr\"\n    WORKING_DIR = \"/kaggle/working\"\n    \n    REPO_URL = \"https://github.com/Othmane999/WWW2025_MMCTR_Challenge\"\n    REPO_DIR = os.path.join(WORKING_DIR, \"WWW2025_MMCTR_Challenge\")\n    DATASET_ID = \"MicroLens_1M_x1\"\n    DEST_DATA_DIR = os.path.join(REPO_DIR, \"data\", DATASET_ID)\n    \n    FINAL_SUBMISSION = os.path.join(WORKING_DIR, \"prediction_task1&2.csv\")\n    BEST_MODEL_DEST = os.path.join(WORKING_DIR, \"best_model_task1_and_2_tuned.pth\")\n    \n    # Hyperparameters\n    EPOCHS = 8\n    EARLY_STOP_PATIENCE = 4\n    LEARNING_RATE = 0.0005\n    BATCH_SIZE_TRAIN = 4096\n    BATCH_SIZE_INFERENCE = 4096\n    \n    # Architecture\n    DNN_HIDDEN_UNITS = [2048, 1024, 512, 256]\n    ATTENTION_HIDDEN_UNITS = [1024, 512, 256]\n    EMBEDDING_DIM = 128\n    ATTENTION_DROPOUT = 0.2\n    NET_DROPOUT = 0.2\n    EMBEDDING_REGULARIZER = 5e-7 \n\n# ====================================================\n# 1. NUMBA JIT FUNCTIONS\n# ====================================================\nif NUMBA_AVAILABLE:\n    @njit(parallel=True, fastmath=True)\n    def check_embedding_integrity_jit(emb_matrix):\n        rows, cols = emb_matrix.shape\n        has_error = False\n        for i in prange(rows):\n            if has_error: break \n            for j in range(cols):\n                val = emb_matrix[i, j]\n                if np.isnan(val) or np.isinf(val):\n                    has_error = True\n        return not has_error\n    \n    @njit(fastmath=True)\n    def normalize_embeddings_jit(emb_matrix):\n        rows, cols = emb_matrix.shape\n        normalized = np.zeros_like(emb_matrix)\n        for i in range(rows):\n            norm = 0.0\n            for j in range(cols):\n                norm += emb_matrix[i, j] ** 2\n            norm = np.sqrt(norm)\n            if norm > 1e-8:\n                for j in range(cols):\n                    normalized[i, j] = emb_matrix[i, j] / norm\n            else:\n                for j in range(cols):\n                    normalized[i, j] = emb_matrix[i, j]\n        return normalized\nelse:\n    def check_embedding_integrity_jit(emb_matrix):\n        return not (np.isnan(emb_matrix).any() or np.isinf(emb_matrix).any())\n    \n    def normalize_embeddings_jit(emb_matrix):\n        norms = np.linalg.norm(emb_matrix, axis=1, keepdims=True)\n        norms = np.where(norms > 1e-8, norms, 1.0)\n        return emb_matrix / norms\n\n# ====================================================\n# 2. SETUP\n# ====================================================\ndef setup_environment_task2():\n    print(\"\\n\" + \"=\"*70)\n    print(f\"=== Phase 1: Setup ===\")\n    print(\"=\"*70)\n    \n    if not os.path.exists(Config.REPO_DIR):\n        print(f\"[INFO] Cloning {Config.REPO_URL}...\")\n        os.system(f\"git clone {Config.REPO_URL} {Config.REPO_DIR}\")\n    \n    print(\"[INFO] Installing requirements...\")\n    os.system(f\"pip install -q -r {Config.REPO_DIR}/requirements.txt\")\n    print(\"‚úÖ Environment ready.\")\n\n# ====================================================\n# 3. DATA PREPARATION (MODIFIED)\n# ====================================================\ndef prepare_data_optimized():\n    print(\"\\n\" + \"=\"*70)\n    print(\"=== Phase 2: Data Preparation (Column Swap) ===\")\n    print(\"=\"*70)\n\n    # A. Migration Datasets\n    os.makedirs(Config.DEST_DATA_DIR, exist_ok=True)\n    files = [\"train.parquet\", \"valid.parquet\", \"test.parquet\", \"item_seq.parquet\"]\n    \n    for f in tqdm(files, desc=\"üìÇ Migration Datasets\"):\n        src = os.path.join(Config.SOURCE_DATA_DIR, f)\n        dst = os.path.join(Config.DEST_DATA_DIR, f)\n        if os.path.exists(src): \n            shutil.copy(src, dst)\n\n    # B. Load Custom Item Info\n    print(f\"[INFO] Loading Custom File: {Config.CUSTOM_ITEM_INFO_PATH}...\")\n    if not os.path.exists(Config.CUSTOM_ITEM_INFO_PATH):\n        print(f\"üî¥ ERROR: File not found at {Config.CUSTOM_ITEM_INFO_PATH}\")\n        sys.exit(1)\n        \n    df_final = pd.read_parquet(Config.CUSTOM_ITEM_INFO_PATH)\n    \n    # --- SPECIFIC MODIFICATION START ---\n    print(f\"[INFO] Columns before processing: {df_final.columns.tolist()}\")\n\n    # 1. Drop ancient item_emb_d128 if it exists\n    if 'item_emb_d128' in df_final.columns:\n        print(\"üóëÔ∏è Dropping ancient 'item_emb_d128' column...\")\n        df_final.drop(columns=['item_emb_d128'], inplace=True)\n    \n    # 2. Rename item_emb to item_emb_d128\n    if 'item_emb' in df_final.columns:\n        print(\"üîÑ Renaming 'item_emb' to 'item_emb_d128'...\")\n        df_final.rename(columns={'item_emb': 'item_emb_d128'}, inplace=True)\n    else:\n        print(\"üî¥ ERROR: Column 'item_emb' not found! Cannot proceed.\")\n        sys.exit(1)\n        \n    target_emb_col = \"item_emb_d128\"\n    # --- SPECIFIC MODIFICATION END ---\n\n    # Fill NaNs if any\n    null_mask = df_final[target_emb_col].isnull()\n    if null_mask.any():\n        print(f\"‚ö†Ô∏è {null_mask.sum()} items without embeddings. Filling with zeros.\")\n        zero_vec = [0.0] * 128\n        df_final.loc[null_mask, target_emb_col] = pd.Series(\n            [zero_vec] * null_mask.sum(), \n            index=df_final[null_mask].index\n        )\n\n    # D. Normalisation L2\n    emb_matrix = np.stack(df_final[target_emb_col].values).astype(np.float32)\n    \n    if Config.NORMALIZE_EMBEDDINGS:\n        print(\"[INFO] ‚ö° Ensuring L2 Normalization...\")\n        emb_matrix = normalize_embeddings_jit(emb_matrix)\n        df_final[target_emb_col] = list(emb_matrix)\n        print(\"‚úÖ Embeddings normalized.\")\n    \n    # E. Check Integrity\n    print(\"[INFO] Checking matrix integrity...\")\n    if check_embedding_integrity_jit(emb_matrix):\n        print(f\"‚úÖ Matrix shape {emb_matrix.shape} valid.\")\n    else:\n        print(\"üî¥ Error: NaNs/Infs detected in custom embeddings.\")\n        sys.exit(1)\n\n    # F. Save\n    dst_info = os.path.join(Config.DEST_DATA_DIR, \"item_info.parquet\")\n    df_final.to_parquet(dst_info, index=False)\n    print(f\"‚úÖ item_info.parquet generated successfully.\")\n\n# ====================================================\n# 4. TRAINING CONFIG\n# ====================================================\ndef modify_default_config():\n    import yaml\n    \n    default_config = os.path.join(Config.REPO_DIR, \"config\", \"DIN_microlens_mmctr_tuner_config_01.yaml\")\n    \n    if not os.path.exists(default_config):\n        print(f\"‚ö†Ô∏è Config not found: {default_config}\")\n        return None\n    \n    try:\n        with open(default_config, 'r') as f:\n            config = yaml.safe_load(f)\n        \n        # Hyperparams\n        if 'tuner_space' in config:\n            config['tuner_space']['learning_rate'] = [Config.LEARNING_RATE]\n            config['tuner_space']['batch_size'] = [Config.BATCH_SIZE_TRAIN]\n            config['tuner_space']['embedding_dim'] = [Config.EMBEDDING_DIM]\n            config['tuner_space']['dnn_hidden_units'] = [Config.DNN_HIDDEN_UNITS]\n            config['tuner_space']['attention_hidden_units'] = [Config.ATTENTION_HIDDEN_UNITS]\n            config['tuner_space']['attention_dropout'] = [Config.ATTENTION_DROPOUT]\n            config['tuner_space']['net_dropout'] = [Config.NET_DROPOUT]\n            config['tuner_space']['embedding_regularizer'] = [float(Config.EMBEDDING_REGULARIZER)]\n        \n        if 'model_config' in config:\n            config['model_config']['epochs'] = Config.EPOCHS\n            config['model_config']['early_stop_patience'] = Config.EARLY_STOP_PATIENCE\n        \n        optimized_path = default_config.replace('.yaml', '_optimized.yaml')\n        with open(optimized_path, 'w') as f:\n            yaml.dump(config, f, default_flow_style=False, sort_keys=False, allow_unicode=True)\n        \n        return optimized_path\n    \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Error modifying config: {e}\")\n        return None\n\n# ====================================================\n# 5. TRAINING LOOP\n# ====================================================\ndef run_training_optimized():\n    print(\"\\n\" + \"=\"*70)\n    print(\"=== Phase 3: Training ===\")\n    print(\"=\"*70)\n    \n    cwd = os.getcwd()\n    os.chdir(Config.REPO_DIR)\n    \n    optimized_config = modify_default_config()\n    \n    if optimized_config:\n        config_file = os.path.basename(optimized_config)\n        cmd = f\"python run_param_tuner.py --config config/{config_file} --gpu 0\"\n    else:\n        cmd = \"python run_param_tuner.py --config config/DIN_microlens_mmctr_tuner_config_01.yaml --gpu 0\"\n    \n    print(f\"[EXEC] {cmd}\")\n    \n    result = os.system(cmd)\n    os.chdir(cwd)\n    \n    ckpt_dir = os.path.join(Config.REPO_DIR, \"checkpoints\", Config.DATASET_ID)\n    models = glob.glob(os.path.join(ckpt_dir, \"*.model\")) if os.path.exists(ckpt_dir) else []\n    \n    if models:\n        print(\"\\n‚úÖ Training finished! Model saved.\")\n        return True\n    elif result == 0:\n        print(\"\\n‚úÖ Training finished (no model found, check logs).\")\n        return True\n    else:\n        print(\"\\nüî¥ Training failed.\")\n        return False\n\n# ====================================================\n# 6. INFERENCE\n# ====================================================\ndef load_din_class(source_path):\n    if not os.path.exists(source_path): \n        return None\n    try:\n        spec = importlib.util.spec_from_file_location(\"DIN_Custom\", source_path)\n        module = importlib.util.module_from_spec(spec)\n        sys.modules[\"DIN_Custom\"] = module\n        spec.loader.exec_module(module)\n        return getattr(module, \"DIN\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Error loading DIN class: {e}\")\n        return None\n\ndef run_inference_optimized():\n    from fuxictr.features import FeatureMap\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"=== Phase 4: Inference ===\")\n    print(\"=\"*70)\n    \n    # A. Select Best Model\n    ckpt_dir = os.path.join(Config.REPO_DIR, \"checkpoints\", Config.DATASET_ID)\n    \n    if not os.path.exists(ckpt_dir):\n        print(f\"üî¥ Checkpoints not found: {ckpt_dir}\")\n        return\n    \n    models = glob.glob(os.path.join(ckpt_dir, \"*.model\"))\n    if not models:\n        print(\"üî¥ No models found.\")\n        return\n    \n    best_model_path = max(models, key=os.path.getmtime)\n    print(f\"[INFO] Using model: {os.path.basename(best_model_path)}\")\n    shutil.copy(best_model_path, Config.BEST_MODEL_DEST)\n\n    # B. Config\n    params = {\n        \"model_id\": \"DIN_Inference_Optimized\",\n        \"dataset_id\": Config.DATASET_ID,\n        \"data_root\": Config.DEST_DATA_DIR,\n        \"model_root\": Config.WORKING_DIR,\n        \"feature_cols\": [],\n        \"dnn_hidden_units\": Config.DNN_HIDDEN_UNITS,\n        \"attention_hidden_units\": Config.ATTENTION_HIDDEN_UNITS,\n        \"dnn_activations\": \"ReLU\",\n        \"attention_hidden_activations\": \"ReLU\",\n        \"embedding_dim\": Config.EMBEDDING_DIM,\n        \"batch_norm\": True,\n        \"din_use_softmax\": False,\n        \"gpu\": 0,\n        \"task\": \"binary_classification\",\n        \"loss\": \"binary_crossentropy\",\n        \"metrics\": [\"AUC\", \"logloss\"],\n        \"optimizer\": \"adam\",\n        \"learning_rate\": Config.LEARNING_RATE,\n        \"verbose\": 0,\n        \"attention_dropout\": Config.ATTENTION_DROPOUT,\n        \"net_dropout\": Config.NET_DROPOUT\n    }\n\n    # C. Load Map & Model\n    fmap_path = os.path.join(Config.DEST_DATA_DIR, \"feature_map.json\")\n    feature_map = FeatureMap(Config.DATASET_ID, Config.DEST_DATA_DIR)\n    feature_map.load(fmap_path, params)\n    \n    DIN = load_din_class(os.path.join(Config.REPO_DIR, \"src\", \"DIN.py\"))\n    model = DIN(feature_map, **params)\n    model.load_state_dict(torch.load(best_model_path, map_location=\"cuda:0\"))\n    model.to(\"cuda\")\n    model.eval()\n\n    # D. Inference Data\n    print(\"[INFO] Loading Test Data...\")\n    df_test = pd.read_parquet(os.path.join(Config.SOURCE_DATA_DIR, \"test.parquet\"))\n    df_info = pd.read_parquet(os.path.join(Config.DEST_DATA_DIR, \"item_info.parquet\"))\n    df = pd.merge(df_test, df_info, on=\"item_id\", how=\"left\")\n    \n    def pad_tensor(seq_list, max_len):\n        padded = []\n        for s in seq_list:\n            if not isinstance(s, (list, np.ndarray)): s = []\n            s = list(s)\n            if len(s) > max_len: s = s[:max_len]\n            else: s = s + [0]*(max_len - len(s))\n            padded.append(s)\n        return torch.tensor(padded, dtype=torch.long).to(\"cuda\")\n\n    # E. Batch Prediction\n    all_preds = []\n    num_samples = len(df)\n    \n    print(f\"[INFO] Predicting on {num_samples} samples...\")\n    \n    with torch.no_grad():\n        for start_idx in tqdm(range(0, num_samples, Config.BATCH_SIZE_INFERENCE), desc=\"üöÄ Computing\"):\n            end_idx = min(start_idx + Config.BATCH_SIZE_INFERENCE, num_samples)\n            batch_df = df.iloc[start_idx:end_idx]\n            bs = len(batch_df)\n            \n            # Input Dict Construction\n            batch_input = {}\n            if 'user_id' in batch_df.columns:\n                batch_input['user_id'] = torch.tensor(batch_df['user_id'].values, dtype=torch.long).to(\"cuda\")\n            \n            for c in ['likes_level', 'views_level']:\n                if c in batch_df.columns:\n                    batch_input[c] = torch.tensor(batch_df[c].fillna(0).values, dtype=torch.long).to(\"cuda\")\n            \n            # Item Inputs\n            target_ids = torch.tensor(batch_df['item_id'].values, dtype=torch.long).to(\"cuda\")\n            dummy_ids = torch.zeros_like(target_ids)\n            \n            item_input = {}\n            item_input['item_id'] = torch.stack([dummy_ids, target_ids], dim=1).view(-1)\n            \n            if 'item_tags' in batch_df.columns:\n                target_tags = pad_tensor(batch_df['item_tags'].values, 5)\n                dummy_tags = torch.zeros_like(target_tags)\n                item_input['item_tags'] = torch.stack([dummy_tags, target_tags], dim=1).view(-1, 5)\n            \n            # Use the RENAMED column\n            if 'item_emb_d128' in batch_df.columns:\n                embs_np = np.stack(batch_df['item_emb_d128'].values)\n                target_emb = torch.tensor(embs_np, dtype=torch.float32).to(\"cuda\")\n                dummy_emb = torch.zeros_like(target_emb)\n                item_input['item_emb_d128'] = torch.stack([dummy_emb, target_emb], dim=1).view(-1, 128)\n\n            mask = torch.ones((bs, 1)).to(\"cuda\")\n            out = model.forward((batch_input, item_input, mask))\n            pred = out['y_pred'] if isinstance(out, dict) else out\n            all_preds.extend(pred.cpu().numpy().flatten())\n\n    # F. Submission\n    sub = pd.DataFrame({\"ID\": range(len(all_preds)), \"Task1&2\": all_preds})\n    sub.to_csv(Config.FINAL_SUBMISSION, index=False)\n    print(f\"‚úÖ Prediction saved to {Config.FINAL_SUBMISSION}\")\n\n# ====================================================\n# 7. EXECUTION\n# ====================================================\nif __name__ == \"__main__\":\n    setup_environment_task2()\n    prepare_data_optimized()\n    if run_training_optimized():\n        run_inference_optimized()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T20:38:37.092216Z","iopub.execute_input":"2025-12-15T20:38:37.092747Z","iopub.status.idle":"2025-12-15T22:03:12.830171Z","shell.execute_reply.started":"2025-12-15T20:38:37.092728Z","shell.execute_reply":"2025-12-15T22:03:12.829386Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n=== Phase 1: Setup ===\n======================================================================\n[INFO] Cloning https://github.com/Othmane999/WWW2025_MMCTR_Challenge...\n","output_type":"stream"},{"name":"stderr","text":"Cloning into '/kaggle/working/WWW2025_MMCTR_Challenge'...\n","output_type":"stream"},{"name":"stdout","text":"[INFO] Installing requirements...\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 88.1/88.1 kB 3.8 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 42.6/42.6 kB 3.2 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 31.0/31.0 MB 59.9 MB/s eta 0:00:00\n‚úÖ Environment ready.\n\n======================================================================\n=== Phase 2: Data Preparation (Column Swap) ===\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf-polars-cu12 25.6.0 requires polars<1.29,>=1.25, but you have polars 1.0.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"üìÇ Migration Datasets:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad119def4ed44b899864fc4142647e11"}},"metadata":{}},{"name":"stdout","text":"[INFO] Loading Custom File: /kaggle/working/item_info_updated.parquet...\n[INFO] Columns before processing: ['item_id', 'item_tags', 'item_emb_d128', 'item_emb']\nüóëÔ∏è Dropping ancient 'item_emb_d128' column...\nüîÑ Renaming 'item_emb' to 'item_emb_d128'...\n[INFO] ‚ö° Ensuring L2 Normalization...\n‚úÖ Embeddings normalized.\n[INFO] Checking matrix integrity...\n‚úÖ Matrix shape (91718, 128) valid.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/numba/parfors/parfor.py:2395: NumbaPerformanceWarning: \u001b[1m\nprange or pndindex loop will not be executed in parallel due to there being more than one entry to or exit from the loop (e.g., an assertion).\n\u001b[1m\nFile \"../../tmp/ipykernel_47/4018851867.py\", line 64:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/numba/core/typed_passes.py:336: NumbaPerformanceWarning: \u001b[1m\nThe keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n\nTo find out why, try turning on parallel diagnostics, see https://numba.readthedocs.io/en/stable/user/parallel.html#diagnostics for help.\n\u001b[1m\nFile \"../../tmp/ipykernel_47/4018851867.py\", line 60:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\n  warnings.warn(errors.NumbaPerformanceWarning(msg,\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ item_info.parquet generated successfully.\n\n======================================================================\n=== Phase 3: Training ===\n======================================================================\n[EXEC] python run_param_tuner.py --config config/DIN_microlens_mmctr_tuner_config_01_optimized.yaml --gpu 0\n","output_type":"stream"},{"name":"stderr","text":"2025-12-15 20:38:51,266 P1707 INFO FuxiCTR version: 2.3.7\n2025-12-15 20:38:51,266 P1707 INFO Params: {\n    \"accumulation_steps\": \"1\",\n    \"attention_dropout\": \"0.2\",\n    \"attention_hidden_activations\": \"ReLU\",\n    \"attention_hidden_units\": \"[1024, 512, 256]\",\n    \"attention_output_activation\": \"None\",\n    \"batch_norm\": \"True\",\n    \"batch_size\": \"4096\",\n    \"data_format\": \"parquet\",\n    \"data_root\": \"./data/\",\n    \"dataset_id\": \"MicroLens_1M_x1\",\n    \"debug_mode\": \"False\",\n    \"din_use_softmax\": \"False\",\n    \"dnn_activations\": \"ReLU\",\n    \"dnn_hidden_units\": \"[2048, 1024, 512, 256]\",\n    \"early_stop_patience\": \"3\",\n    \"embedding_dim\": \"128\",\n    \"embedding_regularizer\": \"5e-07\",\n    \"epochs\": \"5\",\n    \"eval_steps\": \"None\",\n    \"feature_cols\": \"[{'active': True, 'dtype': 'int', 'name': 'user_id', 'type': 'meta'}, {'active': True, 'dtype': 'int', 'name': 'item_seq', 'type': 'meta'}, {'active': True, 'dtype': 'int', 'name': 'likes_level', 'type': 'categorical', 'vocab_size': 11}, {'active': True, 'dtype': 'int', 'name': 'views_level', 'type': 'categorical', 'vocab_size': 11}, {'active': True, 'dtype': 'int', 'name': 'item_id', 'source': 'item', 'type': 'categorical', 'vocab_size': 91718}, {'active': True, 'dtype': 'int', 'max_len': 5, 'name': 'item_tags', 'source': 'item', 'type': 'sequence', 'vocab_size': 11740}, {'active': True, 'dtype': 'float', 'embedding_dim': 128, 'name': 'item_emb_d128', 'source': 'item', 'type': 'embedding'}]\",\n    \"feature_config\": \"None\",\n    \"feature_specs\": \"None\",\n    \"gpu\": \"0\",\n    \"group_id\": \"user_id\",\n    \"item_info\": \"./data/MicroLens_1M_x1/item_info.parquet\",\n    \"label_col\": \"{'dtype': 'float', 'name': 'label'}\",\n    \"learning_rate\": \"0.0005\",\n    \"loss\": \"binary_crossentropy\",\n    \"max_len\": \"64\",\n    \"metrics\": \"['AUC', 'logloss']\",\n    \"model\": \"DIN\",\n    \"model_id\": \"DIN_MicroLens_1M_x1_001_2cd806c2\",\n    \"model_root\": \"./checkpoints/\",\n    \"monitor\": \"AUC\",\n    \"monitor_mode\": \"max\",\n    \"net_dropout\": \"0.2\",\n    \"net_regularizer\": \"0\",\n    \"num_workers\": \"3\",\n    \"optimizer\": \"adam\",\n    \"pickle_feature_encoder\": \"True\",\n    \"rebuild_dataset\": \"False\",\n    \"save_best_only\": \"True\",\n    \"seed\": \"20242025\",\n    \"shuffle\": \"True\",\n    \"task\": \"binary_classification\",\n    \"test_data\": \"./data/MicroLens_1M_x1/test.parquet\",\n    \"train_data\": \"./data/MicroLens_1M_x1/train.parquet\",\n    \"use_features\": \"None\",\n    \"valid_data\": \"./data/MicroLens_1M_x1/valid.parquet\",\n    \"verbose\": \"1\"\n}\n2025-12-15 20:38:51,269 P1707 INFO Set up feature processor...\n2025-12-15 20:38:51,269 P1707 INFO Fit feature processor...\n2025-12-15 20:38:51,269 P1707 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'user_id', 'type': 'meta'}\n2025-12-15 20:38:51,270 P1707 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'item_seq', 'type': 'meta'}\n2025-12-15 20:38:51,270 P1707 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'likes_level', 'type': 'categorical', 'vocab_size': 11}\n2025-12-15 20:38:51,270 P1707 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'views_level', 'type': 'categorical', 'vocab_size': 11}\n2025-12-15 20:38:51,270 P1707 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'item_id', 'source': 'item', 'type': 'categorical', 'vocab_size': 91718}\n2025-12-15 20:38:51,295 P1707 INFO Processing column: {'active': True, 'dtype': 'int', 'max_len': 5, 'name': 'item_tags', 'source': 'item', 'type': 'sequence', 'vocab_size': 11740}\n2025-12-15 20:38:51,298 P1707 INFO Processing column: {'active': True, 'dtype': 'float', 'embedding_dim': 128, 'name': 'item_emb_d128', 'source': 'item', 'type': 'embedding'}\n2025-12-15 20:38:51,298 P1707 INFO Set column index...\n2025-12-15 20:38:51,298 P1707 INFO Save feature_map to json: ./data/MicroLens_1M_x1/feature_map.json\n2025-12-15 20:38:51,298 P1707 INFO Pickle feature_encode: ./data/MicroLens_1M_x1/feature_processor.pkl\n2025-12-15 20:38:51,303 P1707 INFO Save feature_vocab to json: ./data/MicroLens_1M_x1/feature_vocab.json\n2025-12-15 20:38:51,445 P1707 INFO Set feature processor done.\n2025-12-15 20:38:51,445 P1707 INFO Load feature_map from json: ./data/MicroLens_1M_x1/feature_map.json\n2025-12-15 20:38:51,445 P1707 INFO Set column index...\n2025-12-15 20:38:51,445 P1707 INFO Feature specs: {\n    \"item_emb_d128\": \"{'source': 'item', 'type': 'embedding', 'embedding_dim': 128}\",\n    \"item_id\": \"{'source': 'item', 'type': 'categorical', 'padding_idx': 0, 'vocab_size': 91718}\",\n    \"item_seq\": \"{'type': 'meta'}\",\n    \"item_tags\": \"{'source': 'item', 'type': 'sequence', 'feature_encoder': 'layers.MaskedAveragePooling()', 'padding_idx': 0, 'max_len': 5, 'vocab_size': 11740}\",\n    \"likes_level\": \"{'source': '', 'type': 'categorical', 'padding_idx': 0, 'vocab_size': 11}\",\n    \"user_id\": \"{'type': 'meta'}\",\n    \"views_level\": \"{'source': '', 'type': 'categorical', 'padding_idx': 0, 'vocab_size': 11}\"\n}\n2025-12-15 20:38:52,978 P1707 INFO Total number of parameters: 20353538.\n2025-12-15 20:38:52,978 P1707 INFO Loading datasets...\n2025-12-15 20:39:02,720 P1707 INFO Train samples: total/3600000, blocks/1\n2025-12-15 20:39:03,054 P1707 INFO Validation samples: total/10000, blocks/1\n2025-12-15 20:39:03,054 P1707 INFO Loading train and validation data done.\n2025-12-15 20:39:03,054 P1707 INFO Start training: 879 batches/epoch\n2025-12-15 20:39:03,054 P1707 INFO ************ Epoch=1 start ************\n","output_type":"stream"},{"name":"stdout","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 878/879 [16:48<00:01,  1.16s/it]","output_type":"stream"},{"name":"stderr","text":"2025-12-15 20:55:52,346 P1707 INFO Train loss: 0.136696\n2025-12-15 20:55:52,346 P1707 INFO Evaluation @epoch 1 - batch 879: \n","output_type":"stream"},{"name":"stdout","text":"\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:02,  1.08s/it]\u001b[A\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:01<00:00,  1.44it/s]\u001b[A\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.73it/s]\u001b[A\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 879/879 [16:51<00:00,  1.15s/it]\n  0%|          | 0/879 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stderr","text":"2025-12-15 20:55:54,096 P1707 INFO [Metrics] AUC: 0.808423\n2025-12-15 20:55:54,097 P1707 INFO Save best model: monitor(max)=0.808423\n2025-12-15 20:55:54,273 P1707 INFO ************ Epoch=1 end ************\n","output_type":"stream"},{"name":"stdout","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 878/879 [16:45<00:01,  1.16s/it]","output_type":"stream"},{"name":"stderr","text":"2025-12-15 21:12:40,572 P1707 INFO Train loss: 0.049287\n2025-12-15 21:12:40,573 P1707 INFO Evaluation @epoch 2 - batch 879: \n","output_type":"stream"},{"name":"stdout","text":"\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:02,  1.07s/it]\u001b[A\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:01<00:00,  1.46it/s]\u001b[A\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.75it/s]\u001b[A\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 879/879 [16:48<00:00,  1.70s/it]","output_type":"stream"},{"name":"stderr","text":"2025-12-15 21:12:42,296 P1707 INFO [Metrics] AUC: 0.833927\n2025-12-15 21:12:42,297 P1707 INFO Save best model: monitor(max)=0.833927\n","output_type":"stream"},{"name":"stdout","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 879/879 [16:48<00:00,  1.15s/it]\n  0%|          | 0/879 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stderr","text":"2025-12-15 21:12:42,545 P1707 INFO ************ Epoch=2 end ************\n","output_type":"stream"},{"name":"stdout","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 878/879 [16:44<00:01,  1.15s/it]","output_type":"stream"},{"name":"stderr","text":"2025-12-15 21:29:28,022 P1707 INFO Train loss: 0.030970\n2025-12-15 21:29:28,022 P1707 INFO Evaluation @epoch 3 - batch 879: \n","output_type":"stream"},{"name":"stdout","text":"\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:02,  1.11s/it]\u001b[A\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:01<00:00,  1.42it/s]\u001b[A\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.70it/s]\u001b[A\n","output_type":"stream"},{"name":"stderr","text":"2025-12-15 21:29:29,794 P1707 INFO [Metrics] AUC: 0.834422\n2025-12-15 21:29:29,795 P1707 INFO Save best model: monitor(max)=0.834422\n","output_type":"stream"},{"name":"stdout","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 879/879 [16:47<00:00,  1.15s/it]\n  0%|          | 0/879 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stderr","text":"2025-12-15 21:29:30,051 P1707 INFO ************ Epoch=3 end ************\n","output_type":"stream"},{"name":"stdout","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 878/879 [16:43<00:01,  1.15s/it]","output_type":"stream"},{"name":"stderr","text":"2025-12-15 21:46:14,952 P1707 INFO Train loss: 0.020916\n2025-12-15 21:46:14,953 P1707 INFO Evaluation @epoch 4 - batch 879: \n","output_type":"stream"},{"name":"stdout","text":"\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:02,  1.08s/it]\u001b[A\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:01<00:00,  1.42it/s]\u001b[A\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.70it/s]\u001b[A\n","output_type":"stream"},{"name":"stderr","text":"2025-12-15 21:46:16,728 P1707 INFO [Metrics] AUC: 0.856528\n2025-12-15 21:46:16,728 P1707 INFO Save best model: monitor(max)=0.856528\n","output_type":"stream"},{"name":"stdout","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 879/879 [16:46<00:00,  1.15s/it]\n  0%|          | 0/879 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stderr","text":"2025-12-15 21:46:16,979 P1707 INFO ************ Epoch=4 end ************\n","output_type":"stream"},{"name":"stdout","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 878/879 [16:43<00:01,  1.16s/it]","output_type":"stream"},{"name":"stderr","text":"2025-12-15 22:03:01,564 P1707 INFO Train loss: 0.014530\n2025-12-15 22:03:01,564 P1707 INFO Evaluation @epoch 5 - batch 879: \n","output_type":"stream"},{"name":"stdout","text":"\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:02,  1.07s/it]\u001b[A\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:01<00:00,  1.43it/s]\u001b[A\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.73it/s]\u001b[A\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 879/879 [16:46<00:00,  1.70s/it]","output_type":"stream"},{"name":"stderr","text":"2025-12-15 22:03:03,313 P1707 INFO [Metrics] AUC: 0.869683\n2025-12-15 22:03:03,313 P1707 INFO Save best model: monitor(max)=0.869683\n","output_type":"stream"},{"name":"stdout","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 879/879 [16:46<00:00,  1.15s/it]\n  0%|          | 0/3 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stderr","text":"2025-12-15 22:03:03,549 P1707 INFO ************ Epoch=5 end ************\n2025-12-15 22:03:03,549 P1707 INFO Training finished.\n2025-12-15 22:03:03,549 P1707 INFO Load best model: /kaggle/working/WWW2025_MMCTR_Challenge/checkpoints/MicroLens_1M_x1/DIN_MicroLens_1M_x1_001_2cd806c2.model\n2025-12-15 22:03:03,622 P1707 INFO ****** Validation evaluation ******\n","output_type":"stream"},{"name":"stdout","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stderr","text":"2025-12-15 22:03:05,400 P1707 INFO [Metrics] AUC: 0.869683 - logloss: 1.807775\n","output_type":"stream"},{"name":"stdout","text":"Enumerate all tuner configurations done.\n\n‚úÖ Training finished! Model saved.\n\n======================================================================\n=== Phase 4: Inference ===\n======================================================================\n[INFO] Using model: DIN_MicroLens_1M_x1_001_2cd806c2.model\n[INFO] Loading Test Data...\n[INFO] Predicting on 379142 samples...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"üöÄ Computing:   0%|          | 0/93 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c127dda0bb1a4758b317542ec7711422"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Prediction saved to /kaggle/working/prediction_task1&2.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}